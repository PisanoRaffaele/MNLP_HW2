{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aPFVVgckTWYL"
      },
      "outputs": [],
      "source": [
        "!pip3 install nltk==3.6.5\n",
        "!pip install transformers\n",
        "!pip install sentence_transformers\n",
        "!pip install datasets\n",
        "!pip install happytransformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIOFwFkwV0c3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.wsd import lesk\n",
        "from datasets import load_dataset\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from happytransformer import HappyTextToText, TTSettings\n",
        "\n",
        "import re\n",
        "import random\n",
        "from collections import Counter\n",
        "import spacy\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMvv076bV1ps"
      },
      "outputs": [],
      "source": [
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
        "spacy_model = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECid41G-0kLz"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIMoHoq21HSH"
      },
      "outputs": [],
      "source": [
        "def get_synonym(synset):\n",
        "    synonyms = synset.lemmas()\n",
        "    return synonyms[1].name() if len(synonyms) > 1 and ('_' not in synonyms[1].name()) else None  # use synonyms[1] cause synonyms[0] usually is the original word\n",
        "\n",
        "def get_hypernym(synset):\n",
        "    hypernyms = synset.hypernyms()\n",
        "    return hypernyms[0].lemmas()[0].name() if hypernyms and ('_' not in hypernyms[0].lemmas()[0].name()) else None\n",
        "\n",
        "def get_hyponym(synset):\n",
        "    hyponyms = synset.hyponyms()\n",
        "    return hyponyms[0].lemmas()[0].name() if hyponyms and ('_' not in hyponyms[0].lemmas()[0].name()) else None\n",
        "\n",
        "def get_meronym(synset):\n",
        "    meronyms = synset.part_meronyms()\n",
        "    return meronyms[0].lemmas()[0].name() if meronyms and ('_' not in meronyms[0].lemmas()[0].name()) else None\n",
        "\n",
        "def get_antonym(synset):\n",
        "    for lemma in synset.lemmas():\n",
        "        antonyms = lemma.antonyms()\n",
        "        return antonyms[0].name() if antonyms else None\n",
        "    return None\n",
        "\n",
        "def augment_sentence(sentence_unit, sentence, typeOfAug, POS):\n",
        "    '''\n",
        "        this function substitute one word of 'sentence' applying an augmentation.\n",
        "        typeOfAug specifies the type of augmentation to perform (get_synonym, get_hypernym, get_hyponym, get_meronym)\n",
        "        POS is the element to augment ('NOUN', 'VERB', 'ADJ', ...)\n",
        "    '''\n",
        "    ret = sentence\n",
        "    for annotation in sentence_unit['wsd']['hypothesis']:\n",
        "        if annotation['pos'] == POS:\n",
        "            if annotation['wnSynsetOffset'] != 'O' and annotation['text'] in sentence and not annotation['text'].istitle(): # to avoid proper nouns\n",
        "                synset = wn.synset_from_pos_and_offset(annotation['wnSynsetOffset'][-1], int(annotation['wnSynsetOffset'][:-1]))\n",
        "                changed_word = typeOfAug(synset)\n",
        "                if changed_word:\n",
        "                    ret = sentence.replace(annotation['text'], changed_word)\n",
        "                    break\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y1N_RgTDYXJg"
      },
      "outputs": [],
      "source": [
        "def test_augment():\n",
        "    d = {0: 'syn', 1: 'hyper', 2: 'hypo', 3: 'mer'}\n",
        "    for i, example in enumerate(dataset['train']):\n",
        "        r = random.randint(0, 3)\n",
        "        if r == 0:\n",
        "            s = augment_sentence(example, example['hypothesis'], get_synonym, 'NOUN') # VERB, ADJ, ADV\n",
        "        elif r == 1:\n",
        "            s = augment_sentence(example, example['hypothesis'], get_hypernym, 'NOUN') # ADJ\n",
        "        elif r == 2:\n",
        "            s = augment_sentence(example, example['hypothesis'], get_hyponym, 'NOUN') # ADJ\n",
        "        elif r == 3:\n",
        "            s = augment_sentence(example, example['hypothesis'], get_meronym, 'NOUN')\n",
        "        if s != example['hypothesis']:\n",
        "            print(d[r])\n",
        "            print(example['hypothesis'])\n",
        "            print(s)\n",
        "            print('#####')\n",
        "# test_augment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Pi1-vR3FPBp"
      },
      "outputs": [],
      "source": [
        "def isVerb(example, idx, pos_tag):\n",
        "    '''\n",
        "    This function takes the starting dataset sample,\n",
        "    the idx (position) of the word th check\n",
        "    the pos_tag (part of speech) of the sentence\n",
        "    and returns True if the word is a verb.\n",
        "    '''\n",
        "    if len(example['srl']['hypothesis']['tokens']) <= idx:\n",
        "        return False\n",
        "    text = example['srl']['hypothesis']['tokens'][idx]['rawText']\n",
        "    for elem in pos_tag:\n",
        "        if elem[0] == text:\n",
        "            if elem[1] in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
        "                return True\n",
        "    return False\n",
        "\n",
        "def get_verb_index(example):\n",
        "    '''\n",
        "    The function returns the indices of the verbs inside the sentence.\n",
        "    '''\n",
        "    ret = []\n",
        "    pos_tag = nltk.pos_tag(nltk.word_tokenize(example['hypothesis']))\n",
        "\n",
        "    for v in example['srl']['hypothesis']['annotations']:\n",
        "      verb_indices = []\n",
        "      pred = 0\n",
        "      for j, elem in enumerate(v['verbatlas']['roles']):\n",
        "          for i in range(pred, elem['span'][0]):\n",
        "              if isVerb(example, i, pos_tag):\n",
        "                  verb_indices.append(i)\n",
        "          if j == len(v['verbatlas']['roles']) - 1:\n",
        "              for i in range(elem['span'][1], len(pos_tag)):\n",
        "                  if isVerb(example, i, pos_tag):\n",
        "                      verb_indices.append(i)\n",
        "          pred = elem['span'][1]\n",
        "      ret.append(verb_indices)\n",
        "    return ret\n",
        "\n",
        "def get_roles_indices(example):\n",
        "    '''\n",
        "    This function returns the indices of the roles inside the sentence.\n",
        "    it is a list of lists, each list is associated to a verb with inside the indices of the roles associated to that verb\n",
        "    '''\n",
        "    indices = []\n",
        "    for v in example['srl']['hypothesis']['annotations']:\n",
        "      r = []\n",
        "      for elem in v['verbatlas']['roles']:\n",
        "          r.append(elem['span'])\n",
        "      indices.append(r)\n",
        "    return indices\n",
        "\n",
        "def get_roles(example):\n",
        "    '''\n",
        "    This function returns the roles inside the sentence.\n",
        "    it is a list of lists, each list is associated to a verb with inside the roles associated to that verb\n",
        "    '''\n",
        "    roles = []\n",
        "    for v in example['srl']['hypothesis']['annotations']:\n",
        "      r = []\n",
        "      for elem in v['verbatlas']['roles']:\n",
        "          r.append(str(elem['role']))\n",
        "      roles.append(r)\n",
        "    return roles\n",
        "\n",
        "def get_index(example, word):\n",
        "    '''\n",
        "    This function returns the index of the word inside the sentence.\n",
        "    '''\n",
        "    example['srl']['hypothesis']['tokens']\n",
        "    for i, token in enumerate(example['srl']['hypothesis']['tokens']):\n",
        "        if token['rawText'] == word:\n",
        "            return i\n",
        "    return -1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00nT2x4imPtE"
      },
      "outputs": [],
      "source": [
        "def flip_verb(example):\n",
        "  '''\n",
        "  This function tryes to flip the verb in the sentence using its antyom or by 'manual' substituion to specific verbs\n",
        "  externally the label is flipped too (contradicts->entailment and vice-versa), and the sentences with NEUTRAL label are skipped\n",
        "  '''\n",
        "  new_sentence = example['hypothesis']\n",
        "  l_roles = get_roles(example)\n",
        "  verbs_indices = get_verb_index(example)\n",
        "\n",
        "  if example['label'] == 'NEUTRAL': # i'm flipping the sense og the sentence to pass from CONTRADICTION to ENTAILMENT and viceversa.\n",
        "      return new_sentence\n",
        "\n",
        "  if len(verbs_indices) == 1 and len(verbs_indices[0]) == 1 and not any('Negation' in roles for roles in l_roles):\n",
        "      ann = example['wsd']['hypothesis'][verbs_indices[0][0]]\n",
        "      if ann['wnSynsetOffset'] != 'O':\n",
        "          synset = wn.synset_from_pos_and_offset(ann['wnSynsetOffset'][-1], int(ann['wnSynsetOffset'][:-1]))\n",
        "          antyom = get_antonym(synset)\n",
        "          if antyom:\n",
        "              new_sentence = new_sentence.replace(ann['text'], antyom)\n",
        "              return new_sentence\n",
        "\n",
        "  # avoids sentences with many verbs to avoid difficult cases that could lead to wrong generation\n",
        "  verbs_count = 0\n",
        "  for verb_indices in verbs_indices:\n",
        "      if len(verb_indices) > 0:\n",
        "        verbs_count +=  1\n",
        "  if verbs_count > 1:\n",
        "      return new_sentence\n",
        "\n",
        "  if ' is not ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' is not ', \" is \")\n",
        "  elif ' is ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' is ', \" is not \")\n",
        "  elif \" isn't \" in new_sentence:\n",
        "      new_sentence = new_sentence.replace(\" isn't \", \" is \")\n",
        "  elif ' are not ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' are not ', \" are \")\n",
        "  elif ' are ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' are ', \" are not \")\n",
        "  elif \" aren't \" in new_sentence:\n",
        "      new_sentence = new_sentence.replace(\" aren't \", \" are \")\n",
        "  elif ' was not ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' was not ', \" was \")\n",
        "  elif ' was ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' was ', \" was not \")\n",
        "  elif \" wasn't \" in new_sentence:\n",
        "      new_sentence = new_sentence.replace(\" wasn't \", \" was \")\n",
        "  elif ' were not ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' were not ', \" were \")\n",
        "  elif ' were ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' were ', \" were not \")\n",
        "  elif \" weren't \" in new_sentence:\n",
        "      new_sentence = new_sentence.replace(\" weren't \", \" were \")\n",
        "  elif ' has not ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' has not ', \" has \")\n",
        "  elif ' has ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' has ', \" has not \")\n",
        "  elif \" hasn't \" in new_sentence:\n",
        "      new_sentence = new_sentence.replace(\" hasn't \", \" has \")\n",
        "  elif ' have not ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' have not ', \" have \")\n",
        "  elif ' have ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' have ', \" have not \")\n",
        "  elif ' had not ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' had not ', \" had \")\n",
        "  elif ' had ' in new_sentence:\n",
        "      new_sentence = new_sentence.replace(' had ', \" had not \")\n",
        "\n",
        "  return new_sentence\n",
        "\n",
        "def flip_with_anty(example):\n",
        "  '''\n",
        "  this function uses the flip_sentence_verb function to flip the verb and then flips the elem of the sentence that is 'reciving' the action.\n",
        "  This double flip brings in a situation where the label in this case is not flipped, beacause both verb and elem that 'recives' the action are flipped\n",
        "  '''\n",
        "  l_roles = get_roles(example)\n",
        "  new_sentence = flip_verb(example)\n",
        "  if new_sentence == example['hypothesis']:\n",
        "      return example['hypothesis']\n",
        "\n",
        "  ret = new_sentence\n",
        "  indices = []\n",
        "  i = 0\n",
        "  for j, roles in enumerate(l_roles):\n",
        "    if 'Attribute' in roles:\n",
        "        indices = get_roles_indices(example)[j][roles.index('Attribute')] # choose the NOUN to change in 'Attribut' part\n",
        "        break\n",
        "    elif 'Theme' in roles:\n",
        "        indices = get_roles_indices(example)[j][roles.index('Theme')] # choose the NOUN to change in 'Theme' part\n",
        "        break\n",
        "\n",
        "  if len(indices) == 0:\n",
        "      return example['hypothesis']\n",
        "\n",
        "  for annotation in example['wsd']['hypothesis']:\n",
        "      if annotation['pos'] == 'NOUN' and i in list(range(indices[0], indices[1])) and annotation['wnSynsetOffset'] != 'O' and not annotation['text'].istitle():\n",
        "          synset = wn.synset_from_pos_and_offset(annotation['wnSynsetOffset'][-1], int(annotation['wnSynsetOffset'][:-1]))\n",
        "          antonym = get_antonym(synset)\n",
        "          if antonym:\n",
        "              ret = new_sentence.replace(annotation['text'], antonym)\n",
        "              break\n",
        "      i += 1\n",
        "  if ret != new_sentence:\n",
        "      return ret\n",
        "  else:\n",
        "      return example['hypothesis']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q969dbOHZ_Uu"
      },
      "outputs": [],
      "source": [
        "def test_flip():\n",
        "    d = {0: 'flip_with_anty', 1: 'flip_with_anty' , 2: 'flip_verb'}\n",
        "    for i, example in enumerate(dataset['train']):\n",
        "        r = random.randint(0, 2)\n",
        "        if r == 0 or r == 1:\n",
        "            s = flip_with_anty(example) #more difficult to do\n",
        "        elif r == 2:\n",
        "            s = flip_verb(example)\n",
        "        if s != example['hypothesis']:\n",
        "            print(d[r] + ':')\n",
        "            print('   ' + example['hypothesis'])\n",
        "            print('   ' + s)\n",
        "            if r == 2:\n",
        "                print('**FLIP LABEL**')\n",
        "            print('#####')\n",
        "#test_flip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SM9zTb3C-71e"
      },
      "outputs": [],
      "source": [
        "def get_tag(text, pos_tag):\n",
        "    '''\n",
        "    this function returns the tag of the word in the sentence\n",
        "    '''\n",
        "    for elem in pos_tag:\n",
        "        if elem[0] == text:\n",
        "            return elem[1]\n",
        "    return ''\n",
        "\n",
        "def get_verb_infos(example, direct_object):\n",
        "    '''\n",
        "    this function returns the tense, the person and the passive form of the verb in the sentence\n",
        "    the person to return is the person of the direct object the verb refers to, cause we need to pass it to passive\n",
        "\n",
        "    VB Verb, base form\n",
        "    VBD Verb, past tense\n",
        "    VBG Verb, gerund or present participle\n",
        "    VBN Verb, past participle\n",
        "    VBP Verb, non-3rd person singular present\n",
        "    VBZ Verb, 3rd person singular present\n",
        "    '''\n",
        "    verbs_indices = get_verb_index(example)\n",
        "    pos_tag = nltk.pos_tag(nltk.word_tokenize(example['hypothesis']))\n",
        "    l_roles = get_roles(example)\n",
        "    l_roles_indices = get_roles_indices(example)\n",
        "\n",
        "    target_roles_idx = -1\n",
        "    for i, roles in enumerate(l_roles):\n",
        "        if 'Theme' in roles and 'Agent' in roles:\n",
        "            target_roles_idx = i\n",
        "            break\n",
        "        elif 'Patient' in roles and 'Agent' in roles:\n",
        "            target_roles_idx = i\n",
        "            break\n",
        "    if target_roles_idx == -1:\n",
        "        return {}\n",
        "\n",
        "    sentence_list = []\n",
        "    for k in example['srl']['hypothesis']['tokens']:\n",
        "        sentence_list.append(k['rawText'])\n",
        "\n",
        "    #search person -> of the direct object in order to decline correctly the verb\n",
        "    if 'Theme' in l_roles[target_roles_idx]:\n",
        "        theme_idx = l_roles_indices[target_roles_idx][l_roles[target_roles_idx].index('Theme')]\n",
        "    elif 'Patient' in l_roles[target_roles_idx]:\n",
        "        theme_idx = l_roles_indices[target_roles_idx][l_roles[target_roles_idx].index('Patient')]\n",
        "    person = ''\n",
        "    for j in range(theme_idx[0], theme_idx[1]):\n",
        "        if sentence_list[j] == direct_object:\n",
        "            tag = get_tag(sentence_list[j], pos_tag)\n",
        "            if tag == '':\n",
        "                return {}\n",
        "            if tag in ['NNPS', 'NNS']: #plural\n",
        "                person = 'Plural'\n",
        "                break\n",
        "            elif tag in ['NNP', 'NN']: #singular\n",
        "                person = 'Singular'\n",
        "\n",
        "    if person == '':\n",
        "        return {}\n",
        "\n",
        "    #search tense\n",
        "    tense = ''\n",
        "    verb_text = ''\n",
        "    verb_index = verbs_indices[0]\n",
        "    tag = get_tag(sentence_list[verb_index[0]], pos_tag)\n",
        "    if tag in ['VBD', 'VBN']:\n",
        "        tense = 'Past'\n",
        "        verb_text = sentence_list[verb_index[0]]\n",
        "    elif tag in ['VBP', 'VBZ', 'VB']:\n",
        "        tense = 'Present'\n",
        "        verb_text = sentence_list[verb_index[0]]\n",
        "        if tag in ['VBZ'] and verb_text.endswith('s'):\n",
        "            verb_text = verb_text[:-1]\n",
        "        elif tag in ['VBZ'] and verb_text.endswith('es'):\n",
        "            verb_text = verb_text[:-2]\n",
        "    elif tag in ['VBG']:\n",
        "        return {'tense': tense, 'person': person, 'Passive': ''}\n",
        "\n",
        "    if tense == '':\n",
        "        return {}\n",
        "\n",
        "    #search passive\n",
        "    passive = ''\n",
        "    if person == 'Plural':\n",
        "        if tense == 'Present':\n",
        "            passive += 'are ' + verb_text + 'ed'\n",
        "        elif tense == 'Past':\n",
        "            passive += 'were ' + verb_text\n",
        "    else:\n",
        "        if tense == 'Present':\n",
        "            passive += 'is ' + verb_text + 'ed'\n",
        "        elif tense == 'Past':\n",
        "            passive += 'was ' + verb_text\n",
        "\n",
        "    return {'tense': tense, 'person': person, 'Passive': passive}\n",
        "\n",
        "def isProperNoun(X, example):\n",
        "    '''\n",
        "    this function checks if the word passed with X is a proper noun\n",
        "    '''\n",
        "    for annotation in example['wsd']['hypothesis']:\n",
        "        if annotation['text'] == X[0] and annotation['pos'] in ['PROPN']:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "def findDirectObject(example):\n",
        "    '''\n",
        "    this function finds the direct object of the sentence\n",
        "    '''\n",
        "    doc = spacy_model(example['hypothesis'])\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"dobj\":\n",
        "            return token.text\n",
        "    return None\n",
        "\n",
        "\n",
        "def passive_form(example):\n",
        "    '''\n",
        "    this function inverts the sentence froma active to a passive form\n",
        "    The Tense in Enlgish that have verb with only one word are Present Simple, Past Simple, Imperative, Gerund\n",
        "    and there must be a Direct object founded with spacy\n",
        "\n",
        "    a the end there's the use of a grammar model to correct in case of errors in generating the passive form for irregular verbs\n",
        "    ex:\n",
        "    - \"Tennis was avoided by Roger Federer entirely.\" -> \"Tennis was avoided entirely by Roger Federer.\"\n",
        "    - \"An NBA record is holded by the Los Angeles Lakers.\" -> \"An NBA record is held by the Los Angeles Lakers.\"\n",
        "    '''\n",
        "    l_roles = get_roles(example)\n",
        "    l_roles_indices = get_roles_indices(example)\n",
        "    verbs_indices = get_verb_index(example)\n",
        "\n",
        "    # find the right set of roles in the sentece (need to include 'Theme' and 'Agent' or 'Patient' and 'Agent')\n",
        "    target_roles_idx = -1\n",
        "    for i, roles in enumerate(l_roles):\n",
        "        if 'Theme' in roles and 'Agent' in roles:\n",
        "            target_roles_idx = i\n",
        "            break\n",
        "        elif 'Patient' in roles and 'Agent' in roles:\n",
        "            target_roles_idx = i\n",
        "            break\n",
        "    if target_roles_idx == -1:\n",
        "        return example['hypothesis']\n",
        "\n",
        "    roles_indices = l_roles_indices[target_roles_idx]\n",
        "    roles = l_roles[target_roles_idx]\n",
        "\n",
        "    if len(verbs_indices) != 1 or len(verbs_indices[0]) != 1: # avoids verbs compused by many words or sentences with many verbs because can't modify them easly\n",
        "        return example['hypothesis']\n",
        "\n",
        "    if 'Negation' in roles: # it is the case of composed verbs but not detected above\n",
        "        return example['hypothesis']\n",
        "\n",
        "    verb_index = verbs_indices[0][0]\n",
        "\n",
        "    i = roles_indices[roles.index('Agent')]\n",
        "    if 'Theme' in roles:\n",
        "        j = roles_indices[roles.index('Theme')]\n",
        "    elif 'Patient' in roles:\n",
        "        j = roles_indices[roles.index('Patient')]\n",
        "\n",
        "    if i[0] > j[0]:\n",
        "        i, j = j, i\n",
        "\n",
        "    if not (i[1] <= verb_index and verb_index < j[0]):  # verb not between the 2, i want 'X does Y' scheme\n",
        "        return example['hypothesis']\n",
        "\n",
        "    sl = []\n",
        "    for k in example['srl']['hypothesis']['tokens']:\n",
        "        sl.append(k['rawText'])\n",
        "\n",
        "    direct_object = findDirectObject(example)\n",
        "    if direct_object == None: # i'm interested in sentences with a direct object\n",
        "        return example['hypothesis']\n",
        "\n",
        "    p_t = get_verb_infos(example, direct_object)\n",
        "\n",
        "    if p_t == {} or p_t['Passive'] == '':\n",
        "        return example['hypothesis']\n",
        "\n",
        "    start = sl[:i[0]]\n",
        "    X = sl[i[0]:verb_index]\n",
        "    Y = sl[verb_index + 1:j[1]]\n",
        "    end = sl[j[1]:]\n",
        "\n",
        "    V = [p_t['Passive'] + ' by']\n",
        "\n",
        "    if len(start) == 0:\n",
        "        Y[0] = Y[0].capitalize()\n",
        "        if not isProperNoun(X, example):\n",
        "            X[0] = X[0].lower()\n",
        "    ret = ' '.join(start + Y + V + X + end)\n",
        "\n",
        "    if ret.endswith(' .') and not example['hypothesis'].endswith(' .'):\n",
        "        ret = ret[:-2] + '.'\n",
        "    elif ret.endswith(' ?') and not example['hypothesis'].endswith(' ?'):\n",
        "        ret = ret[:-2] + '?'\n",
        "    elif ret.endswith(' !') and not example['hypothesis'].endswith(' !'):\n",
        "        ret = ret[:-2] + '!'\n",
        "\n",
        "    if \" 's\" not in example['hypothesis'] and \" 's\" in ret:\n",
        "        ret = ret.replace(\" 's\", \"'s\")\n",
        "\n",
        "    # corrects the phrases that are wrong (irregular verb built in a bad way)\n",
        "    result = happy_tt.generate_text(\"grammar: \" + ret, args=TTSettings(num_beams=5, min_length=1)).text\n",
        "    return result\n",
        "\n",
        "\n",
        "def passive_generic(example, version):\n",
        "    '''\n",
        "    this function does the same things of the passive_form function but the Agent or the Direct object are 'generalized'.\n",
        "    version parameter specify if Agent or Direct object are generalized.\n",
        "    The label is passed to 'Neutral' externally because the sentence is generalized.\n",
        "\n",
        "    - \"Tennis was avoided by Roger Federer entirely.\" -> \"Tennis was avoided entirely by someone.\"\n",
        "    - \"An NBA record is holded by the Los Angeles Lakers.\" -> \"Something is held by the Los Angeles Lakers.\"\n",
        "    '''\n",
        "\n",
        "    l_roles = get_roles(example)\n",
        "    l_roles_indices = get_roles_indices(example)\n",
        "    verbs_indices = get_verb_index(example)\n",
        "\n",
        "    # find the right set of roles in the sentece (need to include 'Theme' and 'Agent' or 'Patient' and 'Agent')\n",
        "    target_roles_idx = -1\n",
        "    for i, roles in enumerate(l_roles):\n",
        "        if 'Theme' in roles and 'Agent' in roles:\n",
        "            target_roles_idx = i\n",
        "            break\n",
        "        elif 'Patient' in roles and 'Agent' in roles:\n",
        "            target_roles_idx = i\n",
        "            break\n",
        "    if target_roles_idx == -1:\n",
        "        return example['hypothesis']\n",
        "\n",
        "    roles_indices = l_roles_indices[target_roles_idx]\n",
        "    roles = l_roles[target_roles_idx]\n",
        "\n",
        "    if len(verbs_indices) != 1 or len(verbs_indices[0]) != 1: # avoids verbs compused by many words or sentences with many verbs because can't modify them easly\n",
        "        return example['hypothesis']\n",
        "\n",
        "    if 'Negation' in roles: # it is the case of composed verbs but not detected above\n",
        "      return example['hypothesis']\n",
        "\n",
        "    verb_index = verbs_indices[0][0]\n",
        "\n",
        "    i = roles_indices[roles.index('Agent')]\n",
        "    if 'Theme' in roles:\n",
        "        j = roles_indices[roles.index('Theme')]\n",
        "    elif 'Patient' in roles:\n",
        "        j = roles_indices[roles.index('Patient')]\n",
        "\n",
        "    if i[0] > j[0]:\n",
        "        i, j = j, i\n",
        "\n",
        "    if not (i[1] <= verb_index and verb_index < j[0]):  # verb not between the 2, i want 'X does Y' scheme\n",
        "        return example['hypothesis']\n",
        "\n",
        "    sl = []\n",
        "    for k in example['srl']['hypothesis']['tokens']:\n",
        "        sl.append(k['rawText'])\n",
        "\n",
        "    direct_object = findDirectObject(example)\n",
        "    if direct_object == None: # i'm interested in sentences with a direct object\n",
        "        return example['hypothesis']\n",
        "\n",
        "    p_t = get_verb_infos(example, direct_object)\n",
        "\n",
        "    if p_t == {} or p_t['Passive'] == '':\n",
        "        return example['hypothesis']\n",
        "\n",
        "    start = sl[:i[0]]\n",
        "    X = sl[i[0]:verb_index]\n",
        "    Y = sl[verb_index + 1:j[1]]\n",
        "    end = sl[j[1]:]\n",
        "\n",
        "    if version == 'Agent':\n",
        "        X = ['someone']\n",
        "    else:\n",
        "        if p_t['person'] == 'Plural':\n",
        "            Y = ['some things']\n",
        "        else:\n",
        "            Y = ['something']\n",
        "\n",
        "    V = [p_t['Passive'] + ' by']\n",
        "\n",
        "    if len(start) == 0:\n",
        "        Y[0] = Y[0].capitalize()\n",
        "        if not isProperNoun(X, example):\n",
        "            X[0] = X[0].lower()\n",
        "    ret = ' '.join(start + Y + V + X + end)\n",
        "\n",
        "    if ret.endswith(' .') and not example['hypothesis'].endswith(' .'):\n",
        "        ret = ret[:-2] + '.'\n",
        "    elif ret.endswith(' ?') and not example['hypothesis'].endswith(' ?'):\n",
        "        ret = ret[:-2] + '?'\n",
        "    elif ret.endswith(' !') and not example['hypothesis'].endswith(' !'):\n",
        "        ret = ret[:-2] + '!'\n",
        "\n",
        "    if \" 's\" not in example['hypothesis'] and \" 's\" in ret:\n",
        "        ret = ret.replace(\" 's\", \"'s\")\n",
        "\n",
        "    # corrects the phrases that are wrong (irregular verb built in a bad way)\n",
        "    result = happy_tt.generate_text(\"grammar: \" + ret, args=TTSettings(num_beams=5, min_length=1)).text\n",
        "\n",
        "    return(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "75dbHp-GcRe4"
      },
      "outputs": [],
      "source": [
        "def test_passive():\n",
        "    d = {0: 'passive_form', 1: 'generalization-Agent', 2:'generalization-directObject'}\n",
        "    for i, example in enumerate(dataset['train']):\n",
        "        r = random.randint(0, 2)\n",
        "        if r == 0:\n",
        "            s = passive_form(example)\n",
        "        elif r == 1:\n",
        "            s = passive_generic(example, 'Agent')\n",
        "        elif r == 2:\n",
        "            s = passive_generic(example, 'Direct object')\n",
        "        if s != example['hypothesis']:\n",
        "            print(d[r] + ':')\n",
        "            print('   ' + example['hypothesis'])\n",
        "            print('   ' + s)\n",
        "            print('#####')\n",
        "#test_passive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZhOPfJuH5Kq-"
      },
      "outputs": [],
      "source": [
        "def check_for_roles_shuffle(s1, s2):\n",
        "    '''\n",
        "    The func. checks if the 2 strings contains the same words to see if the shuffle didn't delete any word\n",
        "    '''\n",
        "    l1 = nltk.word_tokenize(s1)\n",
        "    l2 = nltk.word_tokenize(s2)\n",
        "    l1 = [x.lower() for x in l1 if x != ',']\n",
        "    l2 = [x.lower() for x in l2 if x != ',']\n",
        "    c1 = Counter(l1)\n",
        "    c2 = Counter(l2)\n",
        "    return c1 == c2\n",
        "\n",
        "def shuffle_roles(example):\n",
        "    '''\n",
        "    this function changes the position of some specific roles of the sentence\n",
        "    in particular it shuffles some roles that doesn't affect the sense of the sentence if moved at random in the sentence like 'Location', 'Time', 'Cause'\n",
        "    '''\n",
        "    s = example['hypothesis']\n",
        "    l_roles = get_roles(example)\n",
        "    l_roles_indices = get_roles_indices(example)\n",
        "    verbs_indices = get_verb_index(example)\n",
        "\n",
        "    # we take as a target the first group of roles non empty\n",
        "    target_roles_idx = -1\n",
        "    for j, roles in enumerate(l_roles):\n",
        "        if len(roles) > 0:\n",
        "            target_roles_idx = j\n",
        "            break\n",
        "    if target_roles_idx == -1:\n",
        "        return example['hypothesis']\n",
        "\n",
        "    roles = l_roles[target_roles_idx]\n",
        "    roles_indices = l_roles_indices[target_roles_idx]\n",
        "    verb_indices = verbs_indices[target_roles_idx]\n",
        "\n",
        "    if len(roles_indices) < 3: # roles not in the database or too few roles for a good result\n",
        "        return s\n",
        "\n",
        "    if not ('Location' in roles or 'Time' in roles or 'Cause' in roles): # i'm interested those parts that can be moved\n",
        "        return s\n",
        "\n",
        "    if ' - ' in example['hypothesis']: # only 3 sentences in all the db have ' - ' and that are not good for this transformation\n",
        "        return example['hypothesis']\n",
        "\n",
        "    # add to the roles list the Verbs\n",
        "    r_i = []\n",
        "    j = 0\n",
        "    for e in list(zip(roles, roles_indices)):\n",
        "      while len(verb_indices) < j and verb_indices[j] < e[1][0]:\n",
        "          r_i.append(['Verb', verb_indices[j]])\n",
        "          j += 1\n",
        "      r_i.append(e)\n",
        "\n",
        "    # remove the part to be shifted\n",
        "    to_move = []\n",
        "    copy_r_i = r_i.copy()\n",
        "    for i in r_i:\n",
        "        if i[0] == 'Location':\n",
        "            to_move = i\n",
        "            copy_r_i.remove(i)\n",
        "            break\n",
        "        elif i[0] == 'Time':\n",
        "            to_move = i\n",
        "            copy_r_i.remove(i)\n",
        "            break\n",
        "        elif i[0] == 'Cause':\n",
        "            to_move = i\n",
        "            copy_r_i.remove(i)\n",
        "            break\n",
        "\n",
        "    # choose a new random position for the sentence part\n",
        "    random.seed(42)\n",
        "    x = random.randint(0, len(copy_r_i))\n",
        "    while x == r_i.index(to_move):\n",
        "        x = random.randint(0, len(copy_r_i))\n",
        "\n",
        "    # create the final sentence\n",
        "    sl = []\n",
        "    for word in example['srl']['hypothesis']['tokens']:\n",
        "        sl.append(word['rawText'])\n",
        "\n",
        "    # the firs word needs to be lower if moved\n",
        "    if not isProperNoun(sl, example):\n",
        "        sl[0] = sl[0].lower()\n",
        "\n",
        "    # find the new starting postition of the element to move\n",
        "    new_start = -1\n",
        "    text_to_move_len = len(sl[to_move[1][0]:to_move[1][1]])\n",
        "    for j, e in enumerate(copy_r_i):\n",
        "        if j == x:\n",
        "            new_start = e[1][0]\n",
        "        elif j == 0 and x == 0:\n",
        "            new_start = 0\n",
        "        elif j == len(copy_r_i) - 1 and x == len(copy_r_i):\n",
        "            new_start = len(sl) - 1\n",
        "\n",
        "    #create the new sentence\n",
        "    text_to_move = ' '.join(sl[to_move[1][0]:to_move[1][1]])\n",
        "    ret = ''\n",
        "    for i, word in enumerate(sl):\n",
        "        if i >= to_move[1][0] and i < to_move[1][1]: #skip the text to move part\n",
        "            continue\n",
        "        if i == new_start: # add the part to be moved by adding before and after commas (except special cases)\n",
        "            if i > 0 and ret[-2:] != ', ' and ret.endswith(' '):\n",
        "                ret = ret[:-1] + ', '\n",
        "            ret += text_to_move\n",
        "            if word not in [',', '.', '?', '!', '-', ':']:\n",
        "                ret += ', '\n",
        "        if i < len(sl) - 1 and (sl[i + 1] not in [',', '.', '?', '!', ':'] or i + 1 == new_start):\n",
        "            ret += word + ' '\n",
        "        else:\n",
        "            ret += word\n",
        "\n",
        "    ret = ret.replace(' - ', '-') # the modification adds incorrectly spaces before and after '-'\n",
        "\n",
        "    if \" 's\" not in example['hypothesis'] and \" 's\" in ret: # the modification adds incorrectly a space before the 's\n",
        "        ret = ret.replace(\" 's\", \"'s\")\n",
        "\n",
        "    # the modification adds incorrectly a space before . ? !\n",
        "    if ret.endswith(' .') and not example['hypothesis'].endswith(' .'):\n",
        "        ret = ret[:-2] + '.'\n",
        "    elif ret.endswith(' ?') and not example['hypothesis'].endswith(' ?'):\n",
        "        ret = ret[:-2] + '?'\n",
        "    elif ret.endswith(' !') and not example['hypothesis'].endswith(' !'):\n",
        "        ret = ret[:-2] + '!'\n",
        "    if s[0].isupper():\n",
        "        ret = ret[0].upper() + ret[1:]\n",
        "\n",
        "    if ret.startswith(', '):\n",
        "        ret = ret[2:]\n",
        "\n",
        "    if ret == s:\n",
        "        return s\n",
        "\n",
        "    r2 = ret.replace(',', '').replace(' ', '')\n",
        "    s2 = s.replace(',', '').replace(' ', '')\n",
        "    # in case that the sentence is the same but the add of ',' or ' ' makes them to seem different\n",
        "    if r2 == s2:\n",
        "        return s\n",
        "\n",
        "    if not check_for_roles_shuffle(ret, s):\n",
        "        return s\n",
        "    return ret"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3Wayp8hdo6O"
      },
      "outputs": [],
      "source": [
        "def test_shuffle_roles():\n",
        "    for i, example in enumerate(dataset['train']):\n",
        "        s = shuffle_roles(example)\n",
        "        if s != example['hypothesis']:\n",
        "            print('   ' + example['hypothesis'])\n",
        "            print('   ' + s)\n",
        "            print('#####')\n",
        "#test_shuffle_roles()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTfiR4ZzCmgf"
      },
      "outputs": [],
      "source": [
        "def make_as_question(example):\n",
        "    '''\n",
        "    this function converts the sentence to a question\n",
        "    '''\n",
        "    l_roles = get_roles(example)\n",
        "    l_roles_indices = get_roles_indices(example)\n",
        "    verbs_indices = get_verb_index(example)\n",
        "\n",
        "    # avoids verbs compused by many words or sentences with many verbs because can't modify them easly\n",
        "    # accept the case of auxiliary verbs (ex: \"Elvis was co-produced by a man.\" verbs_indices = [[], [1]])\n",
        "    if len(verbs_indices) != 1 or len(verbs_indices[0]) != 1:\n",
        "        if not (len(verbs_indices) == 2 and len(verbs_indices[0]) == 0 and len(verbs_indices[1]) == 1):\n",
        "          return example['hypothesis']\n",
        "\n",
        "    if len(verbs_indices) == 2:\n",
        "        verb_index = verbs_indices[1][0]\n",
        "        roles = l_roles[1]\n",
        "        roles_indices = l_roles_indices[1]\n",
        "    else:\n",
        "        verb_index = verbs_indices[0][0]\n",
        "        roles = l_roles[0]\n",
        "        roles_indices = l_roles_indices[0]\n",
        "\n",
        "    if len(roles) == 0:\n",
        "        return example['hypothesis']\n",
        "\n",
        "    if 'Agent' not in roles and 'Theme' not in roles: # case in which the tranformation leads to a meaningfull new sentence\n",
        "        return example['hypothesis']\n",
        "\n",
        "    if 'Negation' in roles: # difficult to deal with\n",
        "        return example['hypothesis']\n",
        "\n",
        "    if 'Agent' in roles:\n",
        "        idx = roles_indices[roles.index('Agent')]\n",
        "    else:\n",
        "        idx = roles_indices[roles.index('Theme')]\n",
        "\n",
        "    if idx[1] > verb_index: # the elem doing the action needs to be before the verb\n",
        "        return example['hypothesis']\n",
        "\n",
        "    # pos tag to recognize the verb\n",
        "    pos_tag = nltk.pos_tag(nltk.word_tokenize(example['hypothesis']))\n",
        "\n",
        "    sentence_list = []\n",
        "    for k in example['srl']['hypothesis']['tokens']:\n",
        "        sentence_list.append(k['rawText'])\n",
        "\n",
        "    # create the new verb text as a question\n",
        "    verb_text = sentence_list[verb_index]\n",
        "    verb_tag = get_tag(verb_text, pos_tag)\n",
        "    start = ''\n",
        "    if verb_tag in ['VBD']:\n",
        "        if verb_text not in ['was', 'were']:\n",
        "            start = 'Did '\n",
        "        else:\n",
        "            start = verb_text.capitalize() + ' '\n",
        "    elif verb_tag in ['VBP', 'VB']:\n",
        "        if verb_text not in [\"'m\", 'are', 'am']:\n",
        "            start = 'Do '\n",
        "        else:\n",
        "            if verb_text == \"'m\":\n",
        "                start = 'Am '\n",
        "            else:\n",
        "                start = verb_text.capitalize() + ' '\n",
        "    elif verb_tag in ['VBZ']:\n",
        "        if verb_text not in ['is']:\n",
        "            if verb_text.endswith('s'):\n",
        "                verb_text = verb_text[:-1]\n",
        "            start = 'Does '\n",
        "        else:\n",
        "            start = verb_text.capitalize() + ' '\n",
        "    else:\n",
        "        return example['hypothesis']\n",
        "\n",
        "\n",
        "    # get agent text\n",
        "    agent = ' '.join(sentence_list[idx[0]:idx[1]])\n",
        "\n",
        "    # create question\n",
        "    if verb_text in ['is', 'are', 'was', 'were']:\n",
        "        ret = start + agent + ' ' + ' '.join(sentence_list[verb_index + 1:])\n",
        "    else:\n",
        "        ret = start + agent + ' ' + verb_text + ' ' + ' '.join(sentence_list[verb_index + 1:])\n",
        "\n",
        "    if ret.endswith(' .') and not example['hypothesis'].endswith(' .'):\n",
        "        ret = ret[:-2] + '?'\n",
        "    elif ret.endswith(' ?') and not example['hypothesis'].endswith(' ?'):\n",
        "        ret = ret[:-2] + '?'\n",
        "    elif ret.endswith(' !') and not example['hypothesis'].endswith(' !'):\n",
        "        ret = ret[:-2] + '?'\n",
        "    else:\n",
        "        ret += '?'\n",
        "\n",
        "    if ' - ' in example['hypothesis']: # the modification adds incorrectly spaces before and after '-'\n",
        "        return example['hypothesis']\n",
        "    ret = ret.replace(' - ', '-')\n",
        "\n",
        "    if \" 's\" not in example['hypothesis'] and \" 's\" in ret: # the modification adds incorrectly a space before the 's\n",
        "        ret = ret.replace(\" 's\", \"'s\")\n",
        "\n",
        "    # apply grammar correction for bad tranforamtions (irregular verbs)\n",
        "    result = happy_tt.generate_text(\"grammar: \" + ret, args=TTSettings(num_beams=5, min_length=1)).text\n",
        "\n",
        "    return result\n",
        "\n",
        "def question_answer_modify(example, new_sentence, change_function, POS, answer):\n",
        "    '''\n",
        "    this function applies an augmentation to a sentence that has been tranformed to a question and can add an answer (Yes or No)\n",
        "    '''\n",
        "\n",
        "    #int the case the answer is No, the label needs to be flipped but it can't be if it is NEUTRAL\n",
        "    if answer == 'No':\n",
        "        if example['label'] == 'NEUTRAL':\n",
        "            return new_sentence\n",
        "\n",
        "    if change_function != None:\n",
        "        s = augment_sentence(example, new_sentence, change_function, POS)\n",
        "        if s != new_sentence:\n",
        "            if answer == 'Yes':\n",
        "                s += ' Yes.'\n",
        "            elif answer == 'No':\n",
        "                s += ' No.'\n",
        "    else:\n",
        "        s = new_sentence\n",
        "        if answer == 'Yes':\n",
        "            s += ' Yes.'\n",
        "        elif answer == 'No':\n",
        "            s += ' No.'\n",
        "    return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Avo0RC2Id7or"
      },
      "outputs": [],
      "source": [
        "def test_question():\n",
        "    d = {0: 'question', 1: 'question_augmented', 2:'question_augmented-yes', 3:'question_augmented-no'}\n",
        "    for i, example in enumerate(dataset['train']):\n",
        "        r = random.randint(0, 3)\n",
        "        s = make_as_question(example)\n",
        "        if r == 1:\n",
        "            s2 = question_answer_modify(example, s, get_synonym, 'NOUN', '') #ADJ\n",
        "            if s2 == s:\n",
        "                continue\n",
        "            s = s2\n",
        "        elif r == 2:\n",
        "            s2 = question_answer_modify(example, s, get_synonym, 'NOUN', 'Yes') #ADJ\n",
        "            if s2 == s:\n",
        "                continue\n",
        "            s = s2\n",
        "        elif r == 3:\n",
        "            s2 = question_answer_modify(example, s, get_synonym, 'NOUN', 'No') #ADJ\n",
        "            if s2 == s:\n",
        "                continue\n",
        "            s = s2\n",
        "        if s != example['hypothesis']:\n",
        "            print(d[r] + ':')\n",
        "            print('   ' + example['hypothesis'])\n",
        "            print('   ' + s)\n",
        "            print('#####')\n",
        "#test_question()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qfno8VWc5x-"
      },
      "outputs": [],
      "source": [
        "def flip_label(example):\n",
        "    if example['label'] == 'CONTRADICTION':\n",
        "        example['label'] = 'ENTAILMENT'\n",
        "    elif example['label'] == 'ENTAILMENT':\n",
        "        example['label'] = 'CONTRADICTION'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwivwnHEKGqY"
      },
      "outputs": [],
      "source": [
        "def sum_subgroup(results, name):\n",
        "    '''\n",
        "    count transformation made for each group\n",
        "    '''\n",
        "    sum = 0\n",
        "    for e in results[name]:\n",
        "        sum += results[name][e]\n",
        "    return sum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttN20yz2RwtZ"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "  'passive' : {\n",
        "    'passive_normal' : 0,\n",
        "    'passive_syn_adj' : 0,\n",
        "    'passive_syn_noun' : 0,\n",
        "    'passive_hype_noun' : 0,\n",
        "    'passive_hypo_noun' : 0,\n",
        "    'passive_agent_gen' : 0,\n",
        "    'passive_directObject_gen' : 0,\n",
        "  },\n",
        "  'flip' : {\n",
        "    'flip_normal' : 0,\n",
        "    'flip_syn_adj' : 0,\n",
        "    'flip_syn_noun' : 0,\n",
        "    'flip_hype_adj' : 0,\n",
        "    'flip_hype_noun' : 0,\n",
        "    'flip_anty' : 0\n",
        "  },\n",
        "  'shuffled_roles' : {\n",
        "    'shuffled_roles_normal' : 0,\n",
        "    'shuffled_roles_syn_adj' : 0,\n",
        "    'shuffled_roles_syn_noun' : 0,\n",
        "    'shuffled_roles_hype_noun' : 0\n",
        "  },\n",
        "  'question' : {\n",
        "    'question_normal' : 0,\n",
        "    'question_syn_adj_yes' : 0,\n",
        "    'question_syn_noun_yes' : 0,\n",
        "    'question_syn_adj_no' : 0,\n",
        "    'question_syn_noun_no' : 0,\n",
        "    'question_syn_noun' : 0,\n",
        "    'question_syn_adj' : 0,\n",
        "    'question_no': 0,\n",
        "    'question_yes': 0\n",
        "  },\n",
        "  'augment' : {\n",
        "    'synonym_adj' : 0,\n",
        "    'synonym_noun' : 0,\n",
        "    'synonym_verb' : 0,\n",
        "    'synonym_adv' : 0,\n",
        "    'hyponym_adj' : 0,\n",
        "    'hyperonym_noun' : 0,\n",
        "    'meronym_noun' : 0\n",
        "  }\n",
        "}\n",
        "\n",
        "not_modified = 0\n",
        "\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "train_dataset = dataset['train']\n",
        "'''\n",
        "this part iterates on the elems of the dataset and applies the transformations based on a specific order (based on most difficult transf. to apply to the easiest)\n",
        "There is also a limit to each tranformation\n",
        "it takes a bit cause some function uses model, it speedup if using a GPU\n",
        "'''\n",
        "with open('train_dataset.json', 'w') as f:\n",
        "  with open('errors.json', 'w') as f2:\n",
        "    for i, elem in enumerate(train_dataset):\n",
        "        if i % 1000 == 0:\n",
        "            print(i)\n",
        "        if sum_subgroup(results, 'passive') < 5000:\n",
        "            result = results['passive']\n",
        "            new_sentence = passive_form(elem)\n",
        "            if new_sentence != elem['hypothesis']:\n",
        "                if result['passive_syn_adj'] < 300:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_synonym, 'ADJ')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['passive_syn_adj'] += 1\n",
        "                        continue\n",
        "                if result['passive_syn_noun'] < 1200:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_synonym, 'NOUN')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['passive_syn_noun'] += 1\n",
        "                        continue\n",
        "                if result['passive_agent_gen'] < 1000:\n",
        "                    augmented_sentence = passive_generic(elem, 'Agent')\n",
        "                    if augmented_sentence != elem['hypothesis']:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        elem['label'] = 'NEUTRAL'\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['passive_agent_gen'] += 1\n",
        "                        continue\n",
        "                if result['passive_directObject_gen'] < 1000:\n",
        "                    augmented_sentence = passive_generic(elem, 'DirectObject')\n",
        "                    if augmented_sentence != elem['hypothesis']:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        elem['label'] = 'NEUTRAL'\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['passive_directObject_gen'] += 1\n",
        "                        continue\n",
        "                if result['passive_hype_noun'] < 300:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_hypernym, 'NOUN')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['passive_hype_noun'] += 1\n",
        "                        continue\n",
        "                if result['passive_hypo_noun'] < 300:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_hyponym, 'NOUN')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['passive_hypo_noun'] += 1\n",
        "                        continue\n",
        "                elem['hypothesis'] = new_sentence\n",
        "                json.dump(elem, f)\n",
        "                f.write('\\n')\n",
        "                result['passive_normal'] += 1\n",
        "                continue\n",
        "\n",
        "        if sum_subgroup(results, 'shuffled_roles') < 5000:\n",
        "            result = results['shuffled_roles']\n",
        "            new_sentence = shuffle_roles(elem)\n",
        "            if new_sentence != elem['hypothesis']:\n",
        "                if result['shuffled_roles_syn_adj'] < 500:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_synonym, 'ADJ')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['shuffled_roles_syn_adj'] += 1\n",
        "                        continue\n",
        "                if result['shuffled_roles_syn_noun'] < 1500:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_synonym, 'NOUN')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['shuffled_roles_syn_noun'] += 1\n",
        "                        continue\n",
        "                if result['shuffled_roles_hype_noun'] < 1000:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_hypernym, 'NOUN')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['shuffled_roles_hype_noun'] += 1\n",
        "                        continue\n",
        "                elem['hypothesis'] = new_sentence\n",
        "                json.dump(elem, f)\n",
        "                f.write('\\n')\n",
        "                result['shuffled_roles_normal'] += 1\n",
        "                continue\n",
        "\n",
        "        if sum_subgroup(results, 'flip') < 19000:\n",
        "            result = results['flip']\n",
        "            new_sentence = flip_verb(elem)\n",
        "            if new_sentence != elem['hypothesis']:\n",
        "                if result['flip_syn_adj'] < 1500:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_synonym, 'ADJ')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        flip_label(elem)\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['flip_syn_adj'] += 1\n",
        "                        continue\n",
        "                if result['flip_syn_noun'] < 4500:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_synonym, 'NOUN')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        flip_label(elem)\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['flip_syn_noun'] += 1\n",
        "                        continue\n",
        "                if result['flip_anty'] < 5500:\n",
        "                    augmented_sentence = flip_with_anty(elem)\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['flip_anty'] += 1\n",
        "                        continue\n",
        "                if result['flip_hype_adj'] < 300:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_hypernym, 'ADJ')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        flip_label(elem)\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['flip_hype_adj'] += 1\n",
        "                        continue\n",
        "                if result['flip_hype_noun'] < 2500:\n",
        "                    augmented_sentence = augment_sentence(elem, new_sentence, get_hypernym, 'NOUN')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        flip_label(elem)\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['flip_hype_noun'] += 1\n",
        "                        continue\n",
        "                elem['hypothesis'] = new_sentence\n",
        "                json.dump(elem, f)\n",
        "                f.write('\\n')\n",
        "                result['flip_normal'] += 1\n",
        "                continue\n",
        "\n",
        "        if sum_subgroup(results, 'question') < 18000:\n",
        "            result = results['question']\n",
        "            new_sentence = make_as_question(elem)\n",
        "            if new_sentence != elem['hypothesis']:\n",
        "                if result['question_syn_adj_no'] < 400:\n",
        "                    augmented_sentence = question_answer_modify(elem, new_sentence, get_synonym, 'ADJ', 'No')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        flip_label(elem)\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['question_syn_adj_no'] += 1\n",
        "                        continue\n",
        "                if result['question_syn_adj_yes'] < 400:\n",
        "                    augmented_sentence = question_answer_modify(elem, new_sentence, get_synonym, 'ADJ', 'Yes')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['question_syn_adj_yes'] += 1\n",
        "                        continue\n",
        "                if result['question_syn_adj'] < 400:\n",
        "                    augmented_sentence = question_answer_modify(elem, new_sentence, get_synonym, 'ADJ', '')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        elem['label'] = 'NEUTRAL'\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['question_syn_adj'] += 1\n",
        "                        continue\n",
        "                if result['question_syn_noun_no'] < 1000:\n",
        "                    augmented_sentence = question_answer_modify(elem, new_sentence, get_synonym, 'NOUN', 'No')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        flip_label(elem)\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['question_syn_noun_no'] += 1\n",
        "                        continue\n",
        "                if result['question_syn_noun_yes'] < 1000:\n",
        "                    augmented_sentence = question_answer_modify(elem, new_sentence, get_synonym, 'NOUN', 'Yes')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['question_syn_noun_yes'] += 1\n",
        "                        continue\n",
        "                if result['question_syn_noun'] < 1000:\n",
        "                    augmented_sentence = question_answer_modify(elem, new_sentence, get_synonym, 'NOUN', '')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        elem['label'] = 'NEUTRAL'\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['question_syn_noun'] += 1\n",
        "                        continue\n",
        "                if result['question_no'] < 1500:\n",
        "                    augmented_sentence = question_answer_modify(elem, new_sentence, None, '', 'No')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        flip_label(elem)\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['question_no'] += 1\n",
        "                        continue\n",
        "                if result['question_yes'] < 1500:\n",
        "                    augmented_sentence = question_answer_modify(elem, new_sentence, None, '', 'Yes')\n",
        "                    if augmented_sentence != new_sentence:\n",
        "                        elem['hypothesis'] = augmented_sentence\n",
        "                        json.dump(elem, f)\n",
        "                        f.write('\\n')\n",
        "                        result['question_yes'] += 1\n",
        "                        continue\n",
        "                elem['hypothesis'] = new_sentence\n",
        "                elem['label'] = 'NEUTRAL'\n",
        "                json.dump(elem, f)\n",
        "                f.write('\\n')\n",
        "                result['question_normal'] += 1\n",
        "                continue\n",
        "        if sum_subgroup(results, 'augment') < 20000:\n",
        "            result = results['augment']\n",
        "            if result['synonym_adj'] < 4000:\n",
        "                new_sentence = augment_sentence(elem, elem['hypothesis'], get_synonym, 'ADJ')\n",
        "                if new_sentence != elem['hypothesis']:\n",
        "                    elem['hypothesis'] = new_sentence\n",
        "                    json.dump(elem, f)\n",
        "                    f.write('\\n')\n",
        "                    result['synonym_adj'] += 1\n",
        "                    continue\n",
        "            if result['synonym_noun'] < 5000:\n",
        "                new_sentence = augment_sentence(elem, elem['hypothesis'], get_synonym, 'NOUN')\n",
        "                if new_sentence != elem['hypothesis']:\n",
        "                    elem['hypothesis'] = new_sentence\n",
        "                    json.dump(elem, f)\n",
        "                    f.write('\\n')\n",
        "                    result['synonym_noun'] += 1\n",
        "                    continue\n",
        "            if result['synonym_verb'] < 5000:\n",
        "                new_sentence = augment_sentence(elem, elem['hypothesis'], get_synonym, 'VERB')\n",
        "                if new_sentence != elem['hypothesis']:\n",
        "                    elem['hypothesis'] = new_sentence\n",
        "                    json.dump(elem, f)\n",
        "                    f.write('\\n')\n",
        "                    result['synonym_verb'] += 1\n",
        "                    continue\n",
        "            if result['synonym_adv'] < 2000:\n",
        "                new_sentence = augment_sentence(elem, elem['hypothesis'], get_synonym, 'ADV')\n",
        "                if new_sentence != elem['hypothesis']:\n",
        "                    elem['hypothesis'] = new_sentence\n",
        "                    json.dump(elem, f)\n",
        "                    f.write('\\n')\n",
        "                    result['synonym_adv'] += 1\n",
        "                    continue\n",
        "            if result['hyponym_adj'] < 2000:\n",
        "                new_sentence = augment_sentence(elem, elem['hypothesis'], get_hyponym, 'ADJ')\n",
        "                if new_sentence != elem['hypothesis']:\n",
        "                    elem['hypothesis'] = new_sentence\n",
        "                    json.dump(elem, f)\n",
        "                    f.write('\\n')\n",
        "                    result['hyponym_adj'] += 1\n",
        "                    continue\n",
        "            if result['hyperonym_noun'] < 5000:\n",
        "                new_sentence = augment_sentence(elem, elem['hypothesis'], get_hypernym, 'NOUN')\n",
        "                if new_sentence != elem['hypothesis']:\n",
        "                    elem['hypothesis'] = new_sentence\n",
        "                    json.dump(elem, f)\n",
        "                    f.write('\\n')\n",
        "                    result['hyperonym_noun'] += 1\n",
        "                    continue\n",
        "            if result['meronym_noun'] < 2000:\n",
        "                new_sentence = augment_sentence(elem, elem['hypothesis'], get_meronym, 'NOUN')\n",
        "                if new_sentence != elem['hypothesis']:\n",
        "                    elem['hypothesis'] = new_sentence\n",
        "                    json.dump(elem, f)\n",
        "                    f.write('\\n')\n",
        "                    result['meronym_noun'] += 1\n",
        "                    continue\n",
        "        not_modified += 1\n",
        "        json.dump(elem, f2)\n",
        "        f2.write('\\n')\n",
        "\n",
        "        # i decided to improve the perf of the model to not include the 1500 not modifed sentences in the file\n",
        "        # json.dump(elem, f)\n",
        "        # f.write('\\n')\n",
        "\n",
        "for x in results:\n",
        "  print(x, results[x])\n",
        "print('not modified', not_modified)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0uXdX21Egkt"
      },
      "outputs": [],
      "source": [
        "print(sum_subgroup(results, 'passive'))\n",
        "print(sum_subgroup(results, 'flip'))\n",
        "print(sum_subgroup(results, 'shuffled_roles'))\n",
        "print(sum_subgroup(results, 'question'))\n",
        "print(sum_subgroup(results, 'augment'))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}