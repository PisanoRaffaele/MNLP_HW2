{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX0LdaC2vVJT"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRJs8Ewa2-un"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "from datasets import load_dataset, concatenate_datasets, load_metric, load_dataset, Dataset\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    EvalPrediction,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-DaggCFgLip"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")\n",
        "adversarial_testset = load_dataset(\"iperbole/adversarial_fever_nli\")\n",
        "label_to_id = {'ENTAILMENT': 0, 'NEUTRAL': 1, 'CONTRADICTION': 2}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "############ In case of training the model on both base dataset and adv-generated dataset ############\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "\n",
        "# adv_dataset = load_dataset('json', data_files='/content/drive/MyDrive/dataset_adv/train_dataset_r.json')\n",
        "\n",
        "# base_trainset = dataset['train']\n",
        "# adv_trainset = adv_dataset['train']\n",
        "# base_trainset.shuffle(seed=42)\n",
        "# adv_trainset.shuffle(seed=42)\n",
        "\n",
        "# adv_trainset = adv_trainset.select(range(int(len(base_trainset)*1/2))) # i'm using half of the adv for better results\n",
        "# train_dataset = concatenate_datasets([adv_trainset, base_trainset])\n",
        "# train_dataset.shuffle(seed=42)\n",
        "\n",
        "########################################################"
      ],
      "metadata": {
        "id": "VDejE9x7E3Zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-asPBZwbpHk"
      },
      "outputs": [],
      "source": [
        "######### base model #########\n",
        "train_dataset = dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDVJcVjPK-jp"
      },
      "outputs": [],
      "source": [
        "language_model_name = \"microsoft/deberta-v3-base\"\n",
        "custom_gsa = 32\n",
        "batch_size = 8\n",
        "learning_rate = 2e-5\n",
        "weight_decay = 0.01\n",
        "max_inpunt_token_len = 512\n",
        "epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnQC7RJhJU1l"
      },
      "outputs": [],
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\", trust_remote_code=True)\n",
        "f1_metric = evaluate.load(\"f1\", trust_remote_code=True)\n",
        "precision_metric = evaluate.load(\"precision\", trust_remote_code=True)\n",
        "recall_metric = evaluate.load(\"recall\", trust_remote_code=True)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')[\"f1\"]\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')[\"recall\"]\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "pos_tag_to_id = {\n",
        "    'NOUN':3, 'PROPN':4, 'SCONJ':5, 'X':6, 'CCONJ':7, 'PUNCT':8, 'AUX':9, 'NUM':10, 'ADP':11, 'DET':12, 'VERB':13, 'INTJ':14, 'ADJ':15, 'SYM':16, 'ADV':17, 'PRON':18, 'PART':19, \"<unk>\": 20\n",
        "    }\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(\n",
        "        examples[\"premise\"],\n",
        "        examples[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        max_length=max_inpunt_token_len,\n",
        "        )\n",
        "\n",
        "    tokens[\"label\"] = [label_to_id[label] for label in examples[\"label\"]]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDC0np3vOjNR"
      },
      "outputs": [],
      "source": [
        "# used for tokenize adv_testset that is without 'wsd' infos\n",
        "def adv_tokenize_function(examples):\n",
        "    tokens = tokenizer(\n",
        "        examples[\"premise\"],\n",
        "        examples[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        max_length=max_inpunt_token_len,\n",
        "        padding='max_length',\n",
        "        )\n",
        "    l = []\n",
        "    for example in examples[\"premise\"]:\n",
        "        l.append([0] * 512)\n",
        "    tokens[\"pos_tags\"] = l\n",
        "\n",
        "    tokens[\"label\"] = [label_to_id[label] for label in examples[\"label\"]]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaeWJQhKtZ1H"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained( language_model_name,\n",
        "                                                            ignore_mismatched_sizes=True,\n",
        "                                                            output_attentions=False,\n",
        "                                                            output_hidden_states=False,\n",
        "                                                            num_labels=3\n",
        "                                                            )\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names, num_proc=2)\n",
        "tokenized_eval_dataset = dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names, num_proc=2)\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArBIUUy_vDPg",
        "outputId": "ea1f014f-ebdd-4f4d-9f7e-94ab5732bab0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"training_dir\",\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=custom_gsa, # Accumulates gradients over custom_gsa steps\n",
        "    warmup_steps=500,                       # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=weight_decay,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    fp16=True,                              # Enable mixed precision training to save memory\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",      # load best model based on best evaluation loss\n",
        "\n",
        "    dataloader_num_workers=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "o4rAJRJNucX0"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],  # Aggiungi EarlyStoppingCallback\n",
        ")\n",
        "torch.cuda.empty_cache()\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpvMaurQvktK"
      },
      "outputs": [],
      "source": [
        "train_result = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vORAQ210H1cm"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_result)\n",
        "trainer.save_state()\n",
        "logs = trainer.state.log_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nnCyvoHepWm"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset[\"test\"]\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"test\"].column_names, num_proc=2)\n",
        "test_result = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
        "trainer.log_metrics(\"test\", test_result)\n",
        "print(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0D3V_w0qb6h"
      },
      "outputs": [],
      "source": [
        "tokenized_adversarial_testset = adversarial_testset.map(adv_tokenize_function, batched=True, remove_columns=['part'], num_proc=2)\n",
        "adversarial_test_result = trainer.evaluate(eval_dataset=tokenized_adversarial_testset)\n",
        "trainer.log_metrics(\"adversarial_test\", adversarial_test_result)\n",
        "print(adversarial_test_result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}