{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dX0LdaC2vVJT"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate -U\n",
        "!pip install datasets\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRJs8Ewa2-un"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Dict\n",
        "from datasets import load_dataset, concatenate_datasets, load_metric, load_dataset\n",
        "from transformers import (\n",
        "    AutoConfig,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    EvalPrediction,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        "    EarlyStoppingCallback,\n",
        "    DataCollatorWithPadding,\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-DaggCFgLip"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"tommasobonomo/sem_augmented_fever_nli\")\n",
        "adversarial_testset = load_dataset(\"iperbole/adversarial_fever_nli\")\n",
        "label_to_id = {'ENTAILMENT': 0, 'NEUTRAL': 1, 'CONTRADICTION': 2}\n",
        "train_dataset = dataset['train']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDVJcVjPK-jp"
      },
      "outputs": [],
      "source": [
        "language_model_name = \"microsoft/deberta-v3-base\"\n",
        "custom_gsa = 32\n",
        "batch_size = 8\n",
        "learning_rate = 4e-5 #4\n",
        "weight_decay = 0.001\n",
        "max_inpunt_token_len = 512\n",
        "epochs = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnQC7RJhJU1l"
      },
      "outputs": [],
      "source": [
        "accuracy_metric = evaluate.load(\"accuracy\", trust_remote_code=True)\n",
        "f1_metric = evaluate.load(\"f1\", trust_remote_code=True)\n",
        "precision_metric = evaluate.load(\"precision\", trust_remote_code=True)\n",
        "recall_metric = evaluate.load(\"recall\", trust_remote_code=True)\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
        "    f1 = f1_metric.compute(predictions=predictions, references=labels, average='weighted')[\"f1\"]\n",
        "    precision = precision_metric.compute(predictions=predictions, references=labels, average='weighted')[\"precision\"]\n",
        "    recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')[\"recall\"]\n",
        "\n",
        "    return {\"accuracy\": accuracy, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
        "\n",
        "\n",
        "pos_tag_to_id = {\n",
        "    'NOUN':3, 'PROPN':4, 'SCONJ':5, 'X':6, 'CCONJ':7, 'PUNCT':8, 'AUX':9, 'NUM':10, 'ADP':11, 'DET':12, 'VERB':13, 'INTJ':14, 'ADJ':15, 'SYM':16, 'ADV':17, 'PRON':18, 'PART':19, \"<unk>\": 20\n",
        "    }\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    tokens = tokenizer(\n",
        "        examples[\"premise\"],\n",
        "        examples[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        max_length=max_inpunt_token_len,\n",
        "        padding='max_length',\n",
        "        )\n",
        "\n",
        "    all_pos_tags_premise = []\n",
        "    all_pos_tags_hypothesis = []\n",
        "\n",
        "    for example in examples[\"wsd\"]:\n",
        "        pos_tags_premise = [pos_tag_to_id.get(token_info[\"pos\"], pos_tag_to_id[\"<unk>\"]) for token_info in example[\"premise\"]]\n",
        "        pos_tags_hypothesis = [pos_tag_to_id.get(token_info[\"pos\"], pos_tag_to_id[\"<unk>\"]) for token_info in example[\"hypothesis\"]]\n",
        "        all_pos_tags_premise.append(pos_tags_premise)\n",
        "        all_pos_tags_hypothesis.append(pos_tags_hypothesis)\n",
        "\n",
        "    #create a singular pos_tag to insert in the token following the schema of the input_ids:\n",
        "    #pos_tag = [1, (... pos of premise ... ), 2, (... pos of hypothesis ... ), 2, 0, 0, 0, ..., 0]\n",
        "    #end zeros are isnerted as padding until len == max_inpunt_token_len\n",
        "    input_ids_batch = tokens[\"input_ids\"]\n",
        "    pos_tags = []\n",
        "    for sample_id in range(len(all_pos_tags_premise)):\n",
        "        j = 0\n",
        "        k = 0\n",
        "        pos_tag_sample = []\n",
        "        new = False\n",
        "        for i in input_ids_batch[sample_id]:\n",
        "            if i == 1:\n",
        "                pos_tag_sample.append(1)\n",
        "            elif i == 2:\n",
        "                pos_tag_sample.append(2)\n",
        "                if not new:\n",
        "                    new = True\n",
        "                else:\n",
        "                    while len(pos_tag_sample) != max_inpunt_token_len:\n",
        "                        pos_tag_sample.append(0)\n",
        "                    pos_tags.append(pos_tag_sample)\n",
        "                    break\n",
        "            else:\n",
        "                if new:\n",
        "                    if len(all_pos_tags_hypothesis[sample_id]) > k:\n",
        "                        pos_tag_sample.append(all_pos_tags_hypothesis[sample_id][k])\n",
        "                        k += 1\n",
        "                    else:\n",
        "                        pos_tag_sample.append(0)\n",
        "                else:\n",
        "                    if len(all_pos_tags_premise[sample_id]) > j:\n",
        "                        pos_tag_sample.append(all_pos_tags_premise[sample_id][j])\n",
        "                        j += 1\n",
        "                    else:\n",
        "                        pos_tag_sample.append(0)\n",
        "\n",
        "    tokens[\"pos_tags\"] = pos_tags\n",
        "\n",
        "    tokens[\"label\"] = [label_to_id[label] for label in examples[\"label\"]]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yDC0np3vOjNR"
      },
      "outputs": [],
      "source": [
        "# used for tokenize adv_testset that is without 'wsd' infos\n",
        "def adv_tokenize_function(examples):\n",
        "    tokens = tokenizer(\n",
        "        examples[\"premise\"],\n",
        "        examples[\"hypothesis\"],\n",
        "        truncation=True,\n",
        "        max_length=max_inpunt_token_len,\n",
        "        padding='max_length',\n",
        "        )\n",
        "    l = []\n",
        "    for example in examples[\"premise\"]:\n",
        "        l.append([0] * 512)\n",
        "    tokens[\"pos_tags\"] = l\n",
        "\n",
        "    tokens[\"label\"] = [label_to_id[label] for label in examples[\"label\"]]\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onk8fbu93Q4T"
      },
      "outputs": [],
      "source": [
        "# pos_tags = set()\n",
        "# for example in dataset['train']:\n",
        "#     for ann in example['wsd']['hypothesis']:\n",
        "#         if ann['pos'] not in pos_tags:\n",
        "#             pos_tags.add(ann['pos'])\n",
        "#     for ann in example['wsd']['premise']:\n",
        "#         if ann['pos'] not in pos_tags:\n",
        "#             pos_tags.add(ann['pos'])\n",
        "\n",
        "# for example in dataset['test']:\n",
        "#     for ann in example['wsd']['hypothesis']:\n",
        "#         if ann['pos'] not in pos_tags:\n",
        "#             pos_tags.add(ann['pos'])\n",
        "#     for ann in example['wsd']['premise']:\n",
        "#         if ann['pos'] not in pos_tags:\n",
        "#             pos_tags.add(ann['pos'])\n",
        "\n",
        "# for example in dataset['validation']:\n",
        "#     for ann in example['wsd']['hypothesis']:\n",
        "#         if ann['pos'] not in pos_tags:\n",
        "#             pos_tags.add(ann['pos'])\n",
        "#     for ann in example['wsd']['premise']:\n",
        "#         if ann['pos'] not in pos_tags:\n",
        "#             pos_tags.add(ann['pos'])\n",
        "\n",
        "# num_pos_tag = len(pos_tags)\n",
        "num_pos_tag = 20\n",
        "# 20 obtained running the code above commented to speedup = 17 + 0, 1, 2 values used for padding e sentence separation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MbBHohryjrNW"
      },
      "outputs": [],
      "source": [
        "class CustomModelForSequenceClassification(nn.Module):\n",
        "    def __init__(self, base_model_name, num_labels):\n",
        "        super(CustomModelForSequenceClassification, self).__init__()\n",
        "        self.base_model = AutoModelForSequenceClassification.from_pretrained(\n",
        "            base_model_name,\n",
        "            num_labels=num_labels,\n",
        "            ignore_mismatched_sizes=True,\n",
        "            output_attentions=False,\n",
        "            output_hidden_states=False\n",
        "        )\n",
        "        self.pos_tag_embeddings = nn.Embedding(num_pos_tag, self.base_model.config.hidden_size)\n",
        "        self.classifier = nn.Linear(self.base_model.config.hidden_size, num_labels)\n",
        "        self.hidden_size = self.base_model.config.hidden_size\n",
        "        self.linear_layer = nn.Linear(self.hidden_size * 2, self.hidden_size).to(device)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, pos_tags=None, labels=None):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        pos_tags = pos_tags.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = self.base_model.base_model(input_ids=input_ids,\n",
        "                                             attention_mask=attention_mask,\n",
        "                                             token_type_ids=token_type_ids)\n",
        "\n",
        "        hidden_states = outputs[0]\n",
        "\n",
        "        pos_tag_embeds = self.pos_tag_embeddings(pos_tags)\n",
        "\n",
        "        combined_embeds = self.linear_layer(torch.cat((hidden_states, pos_tag_embeds), dim=-1))\n",
        "\n",
        "        pooled_output = torch.mean(combined_embeds, dim=1)\n",
        "\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, logits) if loss is not None else logits\n",
        "\n",
        "model = CustomModelForSequenceClassification(\n",
        "    base_model_name=language_model_name,\n",
        "    num_labels=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BaeWJQhKtZ1H"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(language_model_name)\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names, num_proc=2)\n",
        "tokenized_eval_dataset = dataset[\"validation\"].map(tokenize_function, batched=True, remove_columns=dataset[\"train\"].column_names, num_proc=2)\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArBIUUy_vDPg"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"training_dir\",\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=custom_gsa,# Accumulates gradients over custom_gsa steps\n",
        "    warmup_steps=500,                      # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=weight_decay,\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=learning_rate,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=100,\n",
        "    fp16=True,                             # Enable mixed precision training to save memory\n",
        "    report_to=\"none\",\n",
        "    save_total_limit=1,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",     # load best model based on best evaluation loss\n",
        "    dataloader_num_workers=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4rAJRJNucX0"
      },
      "outputs": [],
      "source": [
        "set_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
        ")\n",
        "torch.cuda.empty_cache()\n",
        "trainer.train()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fpvMaurQvktK"
      },
      "outputs": [],
      "source": [
        "train_result = trainer.evaluate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vORAQ210H1cm"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()\n",
        "trainer.log_metrics(\"train\", train_result)\n",
        "trainer.save_state()\n",
        "logs = trainer.state.log_history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nnCyvoHepWm"
      },
      "outputs": [],
      "source": [
        "test_dataset = dataset[\"test\"]\n",
        "tokenized_test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=dataset[\"test\"].column_names, num_proc=2, cache_file_name='test_cache.arrow')\n",
        "test_result = trainer.evaluate(eval_dataset=tokenized_test_dataset)\n",
        "trainer.log_metrics(\"test\", test_result)\n",
        "print(test_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0D3V_w0qb6h"
      },
      "outputs": [],
      "source": [
        "tokenized_adversarial_testset = adversarial_testset.map(adv_tokenize_function, batched=True, remove_columns=['part'], num_proc=2)\n",
        "adversarial_test_result = trainer.evaluate(eval_dataset=tokenized_adversarial_testset)\n",
        "trainer.log_metrics(\"adversarial_test\", adversarial_test_result)\n",
        "print(adversarial_test_result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}